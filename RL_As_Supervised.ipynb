{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL as Classification\n",
    "\n",
    "In this assignment we explore the idea of using classification in Reinforcement Learning algorithms.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Policy Iteration\n",
    "\n",
    "Approximate policy iteration (API), which is an example of approximate dynamic programming, can be used when the state space of the underlying MDP is extremely large (and thus exact methods fail) [2].  \n",
    "Instead of computing the exact improved policy at each step, API uses an approximation to the improved policy.  \n",
    "\n",
    "Recall the policy iteration algorithm, as shown in the schematic below:\n",
    "![alt text](images/exact_PI_flow.png)\n",
    "\n",
    "Instead of computing the value function for each state, we can use rollouts to sample the MDP and obtain a state-action value function approximation which we can use to pick which action maximizes $Q_\\pi(s,a)$ for the current policy.  \n",
    "Once we have this data, we can then use a binary classifier to output the maximizing action for each state. The training data thus has the maximizing action as the label for the state.  \n",
    "We can describe this algorithm using this simple schematic:\n",
    "![alt text](images/approximate_PI_flow.png)\n",
    "\n",
    "This approach brings three main advantages, as written in [4]:\n",
    " - Often, policies are simpler to represent and learn than value functions\n",
    " - A rough estimate of the value function often is sufficient to separate the best action from the rest.\n",
    " - Even if the best action estimates are noisy (due to the value function approximation), the generalization afforded by classification methods usually smooths out the noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an implementation of approximate policy iteration using a simple 2 layers fully connected network on the cartpole task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Reinforcement Learning, an Introduction, 2017 Draft, Sutton, Barto \n",
    "\n",
    "[2] Reinforcement Learning as Classification: Leveraging Modern Classifiers, 2003, Lagoudakis, Parr\n",
    "\n",
    "[3] Oregon State CS533 Class Notes (https://web.engr.oregonstate.edu/~afern/classes/cs533/notes/api.pdf), Fern\n",
    "\n",
    "[4] Classification-based Approximate Policy Iteration: Experiments and Extended Discussions, 2014, Farahmand, Precup, Barreto, Ghavamzadeh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
